# Netflix Keystone: Real-Time Data Backbone

## 1. Problem Statement
With 200M+ users, Netflix cannot wait for overnight batch jobs to know if playback is failing. We need a real-time 'Keystone' pipeline to ingest telemetry (Start, Stop, Buffer) and instantly detect Quality of Experience (QoE) issues per region or device.

## 2. Requirements & KPIs
- High Throughput: Ingest millions of events/second (Heartbeats, Errors).
- Sessionization: Correlate 'Start', 'Buffer', and 'Stop' events into a single viewing session.
- Latency: Detect buffering loops in < 3 seconds.
- Routing: Route data to both Real-time Dashboards (Elasticsearch) and Data Lake (S3).

## 3. Architecture & Pipeline
1. **Edge (Producer)**: Devices emit telemetry events (JSON).
2. **Ingest**: Kafka Topic `keystone-telemetry`.
3. **Processing (Consumer)**: 
   - Maintains 'Active Session' state in memory.
   - Counts 'Buffer' events per user.
4. **Action**: Triggers 'Bitrate Downgrade' alerts if buffering exceeds thresholds.

---

## 4. Technical Implementation

### File Structure
- `producer.py`: Simulates millions of devices sending playback telemetry.
- `consumer.py`: The Keystone Aggregator. It sessionizes events to calculate view time and buffering.
- `utils_logger.py`: High-performance logging config.

### Architecture Diagram: Netflix Keystone Pipeline

```mermaid
%%{init: {'theme': 'neutral', 'themeVariables': { 'fontFamily': 'arial', 'fontSize': '14px'}}}%%
graph TD
    %% Definitions & Styling
    classDef source fill:#E1D5E7,stroke:#9673A6,stroke-width:2px,color:#000;
    classDef ingestion fill:#FFF2CC,stroke:#D6B656,stroke-width:2px,color:#000;
    classDef processing fill:#DAE8FC,stroke:#6C8EBF,stroke-width:2px,color:#000;
    classDef storage fill:#F5F5F5,stroke:#666666,stroke-width:2px,color:#000;
    classDef action fill:#D5E8D4,stroke:#82B366,stroke-width:2px,color:#000;

    subgraph Sources ["Client Telemetry (Edge)"]
        SmartTV["Smart TV<br/>(4K HDR Stream)"]:::source
        Mobile["Mobile Devices<br/>(Variable Bitrate)"]:::source
    end

    subgraph Ingestion ["Keystone Data Backbone"]
        Kafka["Apache Kafka<br/>Topic: keystone-telemetry"]:::ingestion
    end

    subgraph Processing ["Stream Processing (Router)"]
        Aggregator["Keystone Aggregator<br/>(Sessionization Logic)"]:::processing
        StateStore[("In-Memory State<br/>(Active Sessions)")]:::storage
    end

    subgraph RealTime ["Real-Time Path (Hot)"]
        Elastic["Elasticsearch<br/>(Operational Dashboards)"]:::action
        Alerts["Ops Alerting<br/>(Bitrate Downgrade)"]:::action
    end

    subgraph Batch ["Long-Term Path (Cold)"]
        S3[("AWS S3 Data Lake<br/>(Historical Analysis)")]:::storage
    end

    %% Data Flow
    SmartTV -->|"1. Emit Start/Buffer"| Kafka
    Mobile -->|"1. Emit Heartbeat"| Kafka
    Kafka -->|"2. Consume High Volume"| Aggregator

    Aggregator -->|"3. Update Session"| StateStore
    StateStore -.->|"4. Check Thresholds"| Aggregator

    Aggregator -->|"5. Route Critical Events"| Alerts
    Aggregator -->|"5. Index Metrics"| Elastic
    Aggregator -->|"6. Archive Session"| S3

    linkStyle 2,3,4,5,6,7 stroke:#007ACC,stroke-width:2px,fill:none;
```


### How to Run this Demo

**Step 1: Install Dependencies**
```bash
pip install -r requirements.txt
```

**Step 2: Start the Analytics Engine (Consumer)**
This service watches the stream for quality issues.
```bash
python consumer.py
```
*It will simulate a user session where buffering gets critical...*

**Step 3: Start the Traffic Generator (Producer)**
This generates random traffic (Starts, Stops, Heartbeats).
```bash
python producer.py
```

**Step 4: Observe Real-Time QoE**
Watch the Consumer terminal. You will see:
- ‚ñ∂Ô∏è **START**: User began a session.
- ‚ö†Ô∏è **BUFFERING**: Warning logs as lag increases.
- üö® **CRITICAL QoE ALERT**: Triggered when a user buffers 3x in a row.
- ‚èπÔ∏è **STOP**: Calculates total viewing duration when session ends.

---
*Generated by Automation Script | Netflix Keystone: Real-Time Data Backbone Project*
