import os

# ==========================================
# CONFIGURATION: Snapchat AR Use Case
# ==========================================
project_config = {
    "title": "Snapchat: Real-Time ML & Augmented Reality",
    "problem_statement": (
        "Augmented Reality (AR) filters (like dog ears) must 'stick' to a user's face perfectly. "
        "If the processing pipeline takes longer than 33 milliseconds (1 frame at 30fps), "
        "the filter 'drifts' behind the face, breaking the immersion. We need an ultra-low latency "
        "streaming pipeline for ML inference."
    ),
    "requirements": [
        "Hard Real-Time Constraint: Total round-trip time < 33ms.",
        "Throughput: Handle video feature vectors from millions of active users.",
        "Fallibility: It is better to DROP a late frame than to render it late (prevents motion sickness).",
        "Pipeline: Camera -> Feature Extraction -> Stream -> ML Model -> Coordinate Response."
    ],
    "pipeline_description": (
        "1. **Client (Producer)**: Captures video, extracts facial landmarks (eyes, nose), sends JSON stream.\n"
        "2. **Transport**: Kafka (or UDP/gRPC in extreme cases) for high-speed ingest.\n"
        "3. **Inference (Consumer)**: \n"
        "   - Calculates 3D position for the AR asset.\n"
        "   - Checks Total Latency (Network + Compute).\n"
        "4. **Render**: If Latency < 33ms, send coordinates back to phone. Else, discard."
    ),
    "instructions": "python producer.py"
}

# ==========================================
# LOGIC: README Generator
# ==========================================
def generate_readme():
    target_path = "/workspaces/Taashi_Github/Implementation Code/15_Streaming_Big_Data/04_Snapchat_RealTimeML"
    
    # 1. Ensure directory exists
    os.makedirs(target_path, exist_ok=True)
    file_path = os.path.join(target_path, "README.md")
    
    # 2. Formatting Helpers
    req_list = "\n".join([f"- {req}" for req in project_config['requirements']])
    B_BASH = "```bash"
    B_END = "```"
    
    # 3. Construct Content
    content = f"""
# {project_config['title']}

## 1. Problem Statement
{project_config['problem_statement']}

## 2. Requirements & KPIs
{req_list}

## 3. Architecture & Pipeline
{project_config['pipeline_description']}

---

## 4. Technical Implementation

### File Structure
- `producer.py`: Simulates the Camera sending facial landmark vectors (30 FPS).
- `consumer.py`: The ML Inference Engine. Includes logic to measure lag and drop frames.
- `utils_logger.py`: Configured for millisecond-precision logging.

### How to Run this Demo

**Step 1: Install Dependencies**
{B_BASH}
pip install -r requirements.txt
{B_END}

**Step 2: Start the ML Engine (Consumer)**
This service acts as the backend processor calculating AR coordinates.
{B_BASH}
python consumer.py
{B_END}
*The engine will start waiting for video frames...*

**Step 3: Start the Camera Stream (Producer)**
This simulates a user opening Snapchat and moving their head.
{B_BASH}
{project_config['instructions']}
{B_END}

**Step 4: Observe Latency & Drift**
Watch the Consumer terminal. You will see two types of logs:
- ✨ **Rendered Frame**: Success! Processing was fast enough (<33ms).
- ⚠️ **LAG DETECTED**: Failure! The simulated ML model spiked in time, causing the frame to be dropped to prevent "drift."

This demonstrates the "Hard Real-Time" nature of AR streaming.

---
*Generated by Automation Script | {project_config['title']} Project*
"""

    with open(file_path, "w") as f:
        f.write(content.strip())
    
    print(f"✅ README successfully written to:\n   {file_path}")

if __name__ == "__main__":
    generate_readme()